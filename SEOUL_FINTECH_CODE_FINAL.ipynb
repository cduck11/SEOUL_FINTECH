{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3EU5fkJC2R_L",
   "metadata": {
    "id": "3EU5fkJC2R_L"
   },
   "source": [
    "**1. ì‹¤ìŠµë°ì´í„° ê°€ì ¸ì˜¤ê¸°**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68dd551-667d-4021-bd76-a3a2bd465fa0",
   "metadata": {
    "id": "d68dd551-667d-4021-bd76-a3a2bd465fa0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ë‚´ ë“œë¼ì´ë¸Œì— ì‹¤ìŠµìš© ë°ì´í„°ì™€ ì½”ë“œ ì €ì¥\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install -q gdown\n",
    "import gdown, os\n",
    "\n",
    "save_dir = \"/content/drive/MyDrive/Seoul_Fintech\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "url = \"https://drive.google.com/drive/folders/1S-hRk6gwLGpr2hy45hql8Q00IXjczeBo?usp=sharing\"\n",
    "gdown.download_folder(url=url, output=save_dir, quiet=False, use_cookies=False)\n",
    "\n",
    "print(\"êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ë‹¤ìš´ë¡œë“œ ì™„ë£Œ:\", save_dir)\n",
    "!ls -R \"$save_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j-_IFwVRP-kO",
   "metadata": {
    "id": "j-_IFwVRP-kO"
   },
   "source": [
    "**3. ë°ì´í„° ë¶„ì„ ê¸°ì´ˆ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2YSZKJutWmVM",
   "metadata": {
    "id": "2YSZKJutWmVM"
   },
   "source": [
    "(1) pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6NNweyNeP9cJ",
   "metadata": {
    "id": "6NNweyNeP9cJ"
   },
   "outputs": [],
   "source": [
    "# ì‹œë¦¬ì¦ˆ í•™ìŠµí•˜ê¸°\n",
    "\n",
    "import pandas as pd\n",
    "series = pd.Series([1000, 1100, 1200, 1300, 1400, 1500], index=[\"ì‚¬ê³¼\", \"ê·¤\", \"ì˜¤ë Œì§€\", \"ê°\", \"ë°°\", \"ë°”ë‚˜ë‚˜\"])\n",
    "\n",
    "print('ì‹œë¦¬ì¦ˆ ì¶œë ¥')\n",
    "print(\"*\"*30)\n",
    "print(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eAnxSgkMTOiS",
   "metadata": {
    "id": "eAnxSgkMTOiS"
   },
   "outputs": [],
   "source": [
    "# ë°ì´í„° í”„ë ˆì„ í•™ìŠµí•˜ê¸°\n",
    "\n",
    "data = {\n",
    "    \"ì´ë¦„\": [\"ê¹€ì² ìˆ˜\", \"ì´ì˜í¬\", \"ë°•ë¯¼ìˆ˜\"],\n",
    "    \"ë‚˜ì´\": [25, 32, 40],\n",
    "    \"ë³´í—˜ì‚¬\": [\"í•œí™”ìƒëª…\", \"ì‚¼ì„±í™”ì¬\", \"DBì†í•´ë³´í—˜\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MRXnJKsoWrY4",
   "metadata": {
    "id": "MRXnJKsoWrY4"
   },
   "outputs": [],
   "source": [
    "# ë‹¤ìˆ˜ì˜ ë°ì´í„°í”„ë ˆì„ í•™ìŠµí•˜ê¸°\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame({'ì´ë¦„': ['ì² ìˆ˜', 'ì˜í¬'], 'ë³´í—˜ë£Œ': [100, 200]})\n",
    "df2 = pd.DataFrame({'ì´ë¦„': ['ë¯¼ìˆ˜', 'ì§€ìˆ˜'], 'ë³´í—˜ë£Œ': [150, 250]})\n",
    "\n",
    "dfs = [df1, df2]  # ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n",
    "\n",
    "combined = pd.concat(\n",
    "    {\"1ì›”\": df1, \"2ì›”\": df2},\n",
    "    names=[\"ì›”\", \"í–‰ë²ˆí˜¸\"]\n",
    ")\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GgW_YRKsbNj6",
   "metadata": {
    "id": "GgW_YRKsbNj6"
   },
   "source": [
    "(2) NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MnEol8lW7kqP",
   "metadata": {
    "id": "MnEol8lW7kqP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ë°°ì—´ ìƒì„±\n",
    "a = np.array([1, 2, 3, 4, 5])\n",
    "b = np.array([10, 20, 30, 40, 50])\n",
    "\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "\n",
    "# ë°°ì—´ ì—°ì‚°\n",
    "print(\"a + b =\", a + b)\n",
    "print(\"a * b =\", a * b)\n",
    "print(\"a / b =\", a / b)\n",
    "print(\"a ** 2 =\", a ** 2)\n",
    "\n",
    "# ì°¨ì›ê³¼ í˜•íƒœ\n",
    "print(\"ì°¨ì›:\", a.ndim)\n",
    "print(\"í˜•íƒœ:\", a.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Avr4_pl2bRhZ",
   "metadata": {
    "id": "Avr4_pl2bRhZ"
   },
   "outputs": [],
   "source": [
    "# 2ì°¨ì› ë°°ì—´\n",
    "m = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "print(\"m =\\n\", m)\n",
    "\n",
    "# ì „ì¹˜, í•©, í‰ê· \n",
    "print(\"ì „ì¹˜ =\\n\", m.T)\n",
    "print(\"ì „ì²´ í•© =\", np.sum(m))\n",
    "print(\"ì—´ í‰ê·  =\", np.mean(m, axis=0))\n",
    "print(\"í–‰ í‰ê·  =\", np.mean(m, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Tf3BWPOCblQQ",
   "metadata": {
    "id": "Tf3BWPOCblQQ"
   },
   "outputs": [],
   "source": [
    "# ë‚œìˆ˜ ìƒì„±\n",
    "x = np.random.rand(10)  # 0~1 ì‚¬ì´ ëœë¤ 10ê°œ\n",
    "print(\"x =\", x)\n",
    "\n",
    "# í‰ê· , ë¶„ì‚°, í‘œì¤€í¸ì°¨\n",
    "print(\"í‰ê·  =\", np.mean(x))\n",
    "print(\"ë¶„ì‚° =\", np.var(x))\n",
    "print(\"í‘œì¤€í¸ì°¨ =\", np.std(x))\n",
    "\n",
    "# ì •ê·œë¶„í¬ ë‚œìˆ˜\n",
    "z = np.random.randn(10)  # í‰ê·  0, í‘œì¤€í¸ì°¨ 1\n",
    "print(\"ì •ê·œë¶„í¬ z =\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WIu9d8d8dbm_",
   "metadata": {
    "id": "WIu9d8d8dbm_"
   },
   "source": [
    "(3) Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vJ00XvXVbuJm",
   "metadata": {
    "id": "vJ00XvXVbuJm"
   },
   "outputs": [],
   "source": [
    "# ë§·í”Œë¡¯ë¦½ ì–˜ì‚¬ ì½”ë“œ\n",
    "import matplotlib.pyplot as plt\n",
    "# íƒ€ì´í‹€ì…ë ¥\n",
    "plt.title('stuednt')\n",
    "# ë°ì´í„° ì…ë ¥\n",
    "plt.plot([1, 2, 3, 4], [2, 4, 8, 6])\n",
    "plt.plot([1.5, 2.5, 3.5, 4.5], [3, 5, 8, 10])\n",
    "# ë ˆì´ë¸” ì„¤ì •\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Score')\n",
    "# ë²”ë¡€ì…ë ¥\n",
    "plt.legend(['No1 Class', 'No2 Class'])\n",
    "# í‘œ ê·¸ë¦¬ê¸°\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TVoM6oHbzjog",
   "metadata": {
    "id": "TVoM6oHbzjog"
   },
   "outputs": [],
   "source": [
    "# ë‹¨ìˆœ ì˜ˆì¸¡í•˜ê¸° (ë‚˜ì´ â†’ ê³„ì•½ê±´ìˆ˜ ì˜ˆì¸¡)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ì˜ˆì‹œ ë°ì´í„°\n",
    "x = np.array([20, 25, 30, 35, 40, 45, 50])\n",
    "y = np.array([2, 3, 4, 5, 6, 7, 8])\n",
    "\n",
    "# í‰ê·  ê³„ì‚°\n",
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "\n",
    "# ê¸°ìš¸ê¸°(a)ì™€ ì ˆí¸(b) ê³„ì‚° (ë‹¨ìˆœ ì„ í˜•íšŒê·€ ê³µì‹)\n",
    "a = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n",
    "b = y_mean - a * x_mean\n",
    "\n",
    "print(\"ê¸°ìš¸ê¸° a =\", round(a, 3))\n",
    "print(\"ì ˆí¸ b =\", round(b, 3))\n",
    "\n",
    "# ì˜ˆì¸¡ (ìƒˆë¡œìš´ ë‚˜ì´ì— ëŒ€í•œ ê³„ì•½ê±´ìˆ˜)\n",
    "x_new = np.array([60, 70])\n",
    "y_pred = a * x_new + b\n",
    "print(\"ì˜ˆì¸¡ ê²°ê³¼:\", y_pred)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x, y, color='blue', label='ì‹¤ì œ ë°ì´í„°')\n",
    "plt.plot(x, a * x + b, color='red', label='íšŒê·€ì„  (y = a*x + b)')\n",
    "plt.scatter(x_new, y_pred, color='green', marker='x', s=100, label='ì˜ˆì¸¡ê°’')\n",
    "\n",
    "plt.title(\"ë‹¨ìˆœ ì„ í˜•íšŒê·€ ì˜ˆì‹œ: ë‚˜ì´ì— ë”°ë¥¸ ê³„ì•½ê±´ìˆ˜ ì˜ˆì¸¡\", fontsize=13)\n",
    "plt.xlabel(\"ë‚˜ì´\")\n",
    "plt.ylabel(\"ê³„ì•½ê±´ìˆ˜\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A3Yi4KsDzCe3",
   "metadata": {
    "id": "A3Yi4KsDzCe3"
   },
   "source": [
    "(4) ì‹¤ìŠµë°ì´í„°ë¡œ í•™ìŠµí•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IX3L3PWmyrZN",
   "metadata": {
    "id": "IX3L3PWmyrZN"
   },
   "outputs": [],
   "source": [
    "# ì €ì¥ëœ í•™ìŠµë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ\n",
    "csv_path = \"/content/drive/MyDrive/Seoul_Fintech/202201_INSCOMDATA.csv\"\n",
    "\n",
    "# íŒŒì¼ ì½ê¸° (í•œê¸€ ì¸ì½”ë”© ì²˜ë¦¬)\n",
    "df = pd.read_csv(csv_path, encoding='cp949')\n",
    "\n",
    "# ë°ì´í„° ê¸°ë³¸ êµ¬ì¡° í™•ì¸\n",
    "print(\"ë°ì´í„° í¬ê¸° (í–‰, ì—´):\", df.shape)\n",
    "print(\"\\nì»¬ëŸ¼ ëª©ë¡:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# ë°ì´í„° íƒ€ì… ë° ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "print(\"\\në°ì´í„° íƒ€ì… ë° ê²°ì¸¡ì¹˜ ì •ë³´:\")\n",
    "print(df.info())\n",
    "\n",
    "# ìƒ˜í”Œ 5í–‰ í™•ì¸\n",
    "print(\"\\në°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "display(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k_PVmC8MffJM",
   "metadata": {
    "id": "k_PVmC8MffJM"
   },
   "outputs": [],
   "source": [
    "# ë¶ˆëŸ¬ì˜¨ CSVíŒŒì¼ì˜ ë‚´ìš©ì„ ì‹œê°í™” ì¤€ë¹„\n",
    "\n",
    "# ë‚˜ëˆ”/ë…¸í†  CJK í°íŠ¸ ì„¤ì¹˜\n",
    "!apt-get -y install fonts-nanum fonts-nanum-coding fonts-nanum-extra >/dev/null\n",
    "!apt-get -y install fonts-noto-cjk >/dev/null\n",
    "\n",
    "# matplotlibì— í°íŠ¸ ì ìš©\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬\n",
    "df['ì—°ë ¹ëŒ€'] = (df['age_val'] // 10) * 10\n",
    "\n",
    "# ë°ì´í„° í™•ì¸\n",
    "print(df.head(3))\n",
    "print(\"ì „ì²´ í–‰ ìˆ˜:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RxhAD8Awo4cJ",
   "metadata": {
    "id": "RxhAD8Awo4cJ"
   },
   "outputs": [],
   "source": [
    "# ë‹´ë³´ë³„ ê°€ì…ê±´ìˆ˜\n",
    "plt.figure(figsize=(10,12))\n",
    "top_gnt = df.groupby('gnt_itm_nm')['hld_cont_cnt'].sum().sort_values(ascending=False)\n",
    "sns.barplot(x=top_gnt.values, y=top_gnt.index, color=\"skyblue\")\n",
    "plt.title(\"ë‹´ë³´í•­ëª©ë³„ ê°€ì…ê±´ìˆ˜\")\n",
    "plt.xlabel(\"ê°€ì…ê±´ìˆ˜\")\n",
    "plt.ylabel(\"ë‹´ë³´í•­ëª©ëª…\")\n",
    "plt.grid(axis='x', linestyle='--', alpha=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IHCrOJZ21Q6-",
   "metadata": {
    "id": "IHCrOJZ21Q6-"
   },
   "outputs": [],
   "source": [
    "# íšŒì‚¬ë³„ ë‹´ë³´ê°€ì…ê±´ìˆ˜\n",
    "plt.figure(figsize=(10,6))\n",
    "pivot = df.pivot_table(values='hld_cont_cnt', index='inco_nm', columns='gnt_itm_nm', aggfunc='sum', fill_value=0)\n",
    "sns.heatmap(pivot, cmap='YlGnBu')\n",
    "plt.title(\"ë³´í—˜ì‚¬ë³„ ë‹´ë³´í•­ëª©ë³„ ê°€ì…ê±´ìˆ˜ ë¶„í¬(íˆíŠ¸ë§µ)\")\n",
    "plt.xlabel(\"ë‹´ë³´í•­ëª©\")\n",
    "plt.ylabel(\"ë³´í—˜ì‚¬ëª…\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LnDsanJ5JI94",
   "metadata": {
    "id": "LnDsanJ5JI94"
   },
   "source": [
    "**4. ë¨¸ì‹ ëŸ¬ë‹**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WyDpMXiVIG3e",
   "metadata": {
    "id": "WyDpMXiVIG3e"
   },
   "source": [
    "(1) ì‚¬ì´í‚·ëŸ°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R1SG0tgC2GPL",
   "metadata": {
    "id": "R1SG0tgC2GPL"
   },
   "outputs": [],
   "source": [
    "# ì‚¬ì´í‚·ëŸ°ìœ¼ë¡œ ë¨¸ì‹ ëŸ¬ë‹ì„ í†µí•´ ì˜ˆì¸¡í•˜ê¸° (ì‹œí—˜ì ìˆ˜ì˜ˆì¸¡)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ë°ì´í„° ì¤€ë¹„\n",
    "# X: ê³µë¶€í•œ ì‹œê°„ (ì‹œê°„ ë‹¨ìœ„) y: ì‹œí—˜ ì ìˆ˜\n",
    "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
    "y = np.array([40, 45, 50, 55, 60, 65, 70, 78, 85, 90])\n",
    "\n",
    "# í•™ìŠµìš© / í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¶„ë¦¬\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ì„ í˜•íšŒê·€ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ê²°ê³¼:\", y_pred)\n",
    "\n",
    "# ìƒˆë¡œìš´ í•™ìƒì˜ ê³µë¶€ì‹œê°„ ì˜ˆì¸¡ (ì˜ˆ: 7ì‹œê°„, 9ì‹œê°„)\n",
    "new_hours = np.array([[7], [9]])\n",
    "predicted_scores = model.predict(new_hours)\n",
    "print(\"ê³µë¶€ 7ì‹œê°„ â†’ ì˜ˆìƒ ì ìˆ˜:\", round(predicted_scores[0], 1))\n",
    "print(\"ê³µë¶€ 9ì‹œê°„ â†’ ì˜ˆìƒ ì ìˆ˜:\", round(predicted_scores[1], 1))\n",
    "\n",
    "# ì‹œê°í™”\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X, y, color='blue', label='ì‹¤ì œ ë°ì´í„°')\n",
    "plt.plot(X, model.predict(X), color='red', label='ì˜ˆì¸¡ì„  (y = a*x + b)')\n",
    "plt.scatter(new_hours, predicted_scores, color='green', marker='x', s=100, label='ìƒˆë¡œìš´ ì˜ˆì¸¡ê°’')\n",
    "plt.title(\"ê³µë¶€ì‹œê°„ì— ë”°ë¥¸ ì‹œí—˜ ì ìˆ˜ ì˜ˆì¸¡\")\n",
    "plt.xlabel(\"ê³µë¶€ ì‹œê°„ (ì‹œê°„)\")\n",
    "plt.ylabel(\"ì‹œí—˜ ì ìˆ˜\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48TKzldp4fef",
   "metadata": {
    "id": "48TKzldp4fef"
   },
   "source": [
    "(2) ë³´í—˜ê°€ì…ë‹´ë³´ ì˜ˆì¸¡í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9Fqr1mBiOAjN",
   "metadata": {
    "id": "9Fqr1mBiOAjN"
   },
   "outputs": [],
   "source": [
    "# ì‚¬ì´í‚·ëŸ°ìœ¼ë¡œ ë¨¸ì‹ ëŸ¬ë‹ì„ í†µí•´ ì˜ˆì¸¡í•˜ê¸° (ë³´í—˜ê°€ì…ë‹´ë³´ì˜ˆì¸¡ : ëª¨ë¸ë§Œë“¤ê¸°)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "csv_path = \"/content/drive/MyDrive/Seoul_Fintech/202201_INSCOMDATA.csv\"\n",
    "df = pd.read_csv(csv_path, encoding=\"cp949\")\n",
    "\n",
    "# 2. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "use_cols = [\"age_val\", \"sex_cd\", \"gnt_itm_val\", \"gnt_itm_nm\"]\n",
    "df_use = df[use_cols].copy()\n",
    "df_use = df_use.dropna(subset=[\"age_val\", \"sex_cd\", \"gnt_itm_val\"])\n",
    "df_use = df_use[(df_use[\"age_val\"] > 0) & (df_use[\"age_val\"] < 120)]\n",
    "df_use[\"sex_cd\"] = df_use[\"sex_cd\"].astype(str).str.strip()\n",
    "df_use = df_use[df_use[\"sex_cd\"].isin([\"M\", \"F\"])]\n",
    "df_use[\"sex_cd_num\"] = df_use[\"sex_cd\"].map({\"M\": 0, \"F\": 1})\n",
    "\n",
    "\n",
    "# 3. ìƒìœ„ 5ê°œ ë‹´ë³´ì½”ë“œ ì„ íƒ\n",
    "top_codes = df_use[\"gnt_itm_val\"].value_counts().head(5)\n",
    "top_code_list = top_codes.index.tolist()\n",
    "df_top = df_use[df_use[\"gnt_itm_val\"].isin(top_code_list)].copy()\n",
    "\n",
    "\n",
    "# 4. ê· í˜• ìƒ˜í”Œë§ (ê° ë‹´ë³´ì½”ë“œ ë™ì¼ ê°œìˆ˜)\n",
    "code_counts = df_top[\"gnt_itm_val\"].value_counts()\n",
    "min_count = code_counts.min()\n",
    "balanced_list = []\n",
    "for code in top_code_list:\n",
    "    temp = df_top[df_top[\"gnt_itm_val\"] == code].sample(\n",
    "        n=min_count, random_state=42\n",
    "    )\n",
    "    balanced_list.append(temp)\n",
    "df_balanced = pd.concat(balanced_list, axis=0).reset_index(drop=True)\n",
    "\n",
    "# 5. í•™ìŠµë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ë°ì´í„°\n",
    "X = df_balanced[[\"age_val\", \"sex_cd_num\"]].values\n",
    "y = df_balanced[\"gnt_itm_val\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 6. ëª¨ë¸ í•™ìŠµ (ëœë¤í¬ë ˆìŠ¤íŠ¸)\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 7. ì •í™•ë„ í™•ì¸\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜\n",
    "def top_k_accuracy(model, X, y, k):\n",
    "    proba = model.predict_proba(X)\n",
    "    classes = model.classes_\n",
    "    correct = 0\n",
    "    total = len(y)\n",
    "    for i in range(total):\n",
    "        top_k_idx = np.argsort(proba[i])[::-1][:k]\n",
    "        top_k_labels = classes[top_k_idx]\n",
    "        if y[i] in top_k_labels:\n",
    "            correct += 1\n",
    "    return correct / total\n",
    "\n",
    "# 1ê°œ ì •í™•ë„\n",
    "y_pred = model.predict(X_test)\n",
    "top1_acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Top-1 ì •í™•ë„:\", round(top1_acc, 4))\n",
    "# 3ê°œ ì •í™•ë„\n",
    "top3_acc = top_k_accuracy(model, X_test, y_test, k=3)\n",
    "print(\"Top-3 ì •í™•ë„:\", round(top3_acc, 4))\n",
    "# 5ê°œ ì •í™•ë„\n",
    "top5_acc = top_k_accuracy(model, X_test, y_test, k=5)\n",
    "print(\"Top-5 ì •í™•ë„:\", round(top5_acc, 4))\n",
    "print(\"\\nëª¨ë¸ì´ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GL4c2r8g0Den",
   "metadata": {
    "id": "GL4c2r8g0Den"
   },
   "outputs": [],
   "source": [
    "# ì‚¬ì´í‚·ëŸ°ìœ¼ë¡œ ë¨¸ì‹ ëŸ¬ë‹ì„ í†µí•´ ì˜ˆì¸¡í•˜ê¸° (ë³´í—˜ê°€ì…ë‹´ë³´ì˜ˆì¸¡ : ì˜ˆì¸¡í•˜ê¸°)\n",
    "\n",
    "# 1. ë‹´ë³´ì½”ë“œ â†’ ë‹´ë³´ëª… ë§¤í•‘\n",
    "code_name_map = (\n",
    "    df_balanced.groupby([\"gnt_itm_val\", \"gnt_itm_nm\"])\n",
    "               .size()\n",
    "               .reset_index(name=\"cnt\")\n",
    "               .sort_values([\"gnt_itm_val\", \"cnt\"], ascending=[True, False])\n",
    "               .drop_duplicates(subset=[\"gnt_itm_val\"])\n",
    "               .set_index(\"gnt_itm_val\")[\"gnt_itm_nm\"]\n",
    "               .to_dict()\n",
    ")\n",
    "def code_to_name(code):\n",
    "    return code_name_map.get(code, f\"ë‹´ë³´ì½”ë“œ {code}\")\n",
    "\n",
    "# 2. TOP5 ë‹´ë³´ ì¶”ì²œ í•¨ìˆ˜\n",
    "def recommend_top5(age, sex):\n",
    "    sex = str(sex).strip().upper()\n",
    "    sex_num = 0 if sex == \"M\" else 1\n",
    "    X_new = np.array([[age, sex_num]])\n",
    "    # ì˜ˆì¸¡ í™•ë¥  ì „ì²´\n",
    "    proba = model.predict_proba(X_new)[0]\n",
    "    classes = model.classes_\n",
    "    # í™•ë¥  ë†’ì€ ìˆœì„œ 5ê°œ\n",
    "    sorted_idx = np.argsort(proba)[::-1]\n",
    "    top5_idx = sorted_idx[:5]\n",
    "\n",
    "    print(\"\\n========================================\")\n",
    "    print(f\"    ë‚˜ì´ {age}ì„¸, ì„±ë³„ {sex} ê³ ê°ë‹˜ì˜\")\n",
    "    print(\"     ê°€ì… ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹´ë³´ TOP 5\")\n",
    "    print(\"========================================\")\n",
    "\n",
    "    for rank, i in enumerate(top5_idx, start=1):\n",
    "        code = classes[i]\n",
    "        name = code_to_name(code)\n",
    "        p = proba[i]\n",
    "        print(f\"{rank}. {name} (ì½”ë“œ {code}) â†’ í™•ë¥  {p:.3f}\")\n",
    "\n",
    "    print(\"========================================\\n\")\n",
    "\n",
    "\n",
    "# 3. ì˜ˆì¸¡ì‹¤í–‰í•˜ê¸°\n",
    "recommend_top5(25, \"F\")\n",
    "recommend_top5(52, \"M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mn7fshWP4rE1",
   "metadata": {
    "id": "mn7fshWP4rE1"
   },
   "source": [
    "**5. ìì—°ì–´ì²˜ë¦¬**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rldpUADG42Tz",
   "metadata": {
    "id": "rldpUADG42Tz"
   },
   "source": [
    "(1) í† í¬ë‚˜ì´ì§•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q2HzROTQDxZw",
   "metadata": {
    "id": "Q2HzROTQDxZw"
   },
   "outputs": [],
   "source": [
    "#nltk ë¶ˆëŸ¬ì˜¤ê¸° ë° ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sK2V_yhIYofC",
   "metadata": {
    "id": "sK2V_yhIYofC"
   },
   "outputs": [],
   "source": [
    "# ë‹¨ì–´ í† í°í™”\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "print(' 1. ë‹¨ì–´í† í°í™” :',word_tokenize(\"I am actively looking for Ph.D. students. and you are a Ph.D student. Time is an illusion. Lunchtime double so!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KMVLBEbt5AO2",
   "metadata": {
    "id": "KMVLBEbt5AO2"
   },
   "outputs": [],
   "source": [
    "# ë¬¸ì¥ í† í°í™”\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = (\"I am actively looking for Ph.D. students. and you are a Ph.D student. His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\")\n",
    "print(' 2. ë¬¸ì¥í† í°í™” :',sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YbohYICx5n4W",
   "metadata": {
    "id": "YbohYICx5n4W"
   },
   "outputs": [],
   "source": [
    "# ë¬¸ì¥ í† í°í™”\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = (\"\"\"I am actively looking for Ph.D. students. and you are a Ph.D student.\n",
    "His barber kept his word. But keeping such a\n",
    "huge secret to himself was driving him crazy. Finally, the\n",
    "barber went up a mountain and almost to the edge of a\n",
    "cliff. He dug a hole in the midst of some reeds. He looked\n",
    "about, to make sure no one was near.\"\"\")\n",
    "print(' 2. ë¬¸ì¥í† í°í™” :',sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EdNClmO3GLUA",
   "metadata": {
    "id": "EdNClmO3GLUA"
   },
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ ë‹¨ì–´ í† í°í™”\n",
    "print(' 1. ë‹¨ì–´í† í°í™” :',word_tokenize(\"í•œêµ­ì–´ë„ ë‹¨ìœ„ë¡œ í† í°í™”ê°€ ë˜ë‚˜ìš”?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mQWZM9oIMxYi",
   "metadata": {
    "id": "mQWZM9oIMxYi"
   },
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ ë¬¸ì¥ í† í°í™”\n",
    "text = (\"\"\"ìì—°ì–´ì¹˜ëŠ” ìƒë‹¹íˆ ì¬ë¯¸ê°€ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ°ë°, ì¼ë°˜ì ìœ¼ë¡œ ì˜ì–´ëŠ” ì‰¬ìš´ ë°˜ë©´ì—\n",
    "êµì°©ì–´ì¸ í•œêµ­ì–´ëŠ” ë§¤ìš° ì–´ë µìŠµë‹ˆë‹¤. ì´ìœ ëŠ” ë‹¤ì–‘í•œ ë³€í™”í˜•íƒœì™€ ì–´ê°„ ì–´ì ˆêµ¬ì¡°ê°€ ë³µì¡í•˜ê¸° ë•Œë¬¸ì—\n",
    "ìì—°ì–´ì²˜ë¦¬ê°€ ì–´ë ¶ìŠµë‹ˆë‹¤.\"\"\")\n",
    "print(' 2. ë¬¸ì¥í† í°í™” :',sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UAJKLgCpU-kW",
   "metadata": {
    "id": "UAJKLgCpU-kW"
   },
   "source": [
    "(2) í’ˆì‚¬íƒœê¹…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iBgyyPp-U967",
   "metadata": {
    "id": "iBgyyPp-U967"
   },
   "outputs": [],
   "source": [
    "# ì˜ì–´ë¬¸ì¥ í’ˆì‚¬íƒœê¹…\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "\n",
    "print(' ë‹¨ì–´í† í°í™” :',tokenized_sentence)\n",
    "print(' í’ˆì‚¬íƒœê¹… :',pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HBYbTdBANAwa",
   "metadata": {
    "id": "HBYbTdBANAwa"
   },
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ ë¬¸ì¥ í’ˆì‚¬íƒœê¹…\n",
    "!pip install konlpy\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma\n",
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "print('OKT í˜•íƒœì†Œë¶„ì„ :',okt.morphs(\"ì—´ì‹¬íˆ ì½”ë”©í•œ ë‹¹ì‹  , ë‚´ë…„ì—ëŠ” ì–¸ì–´ëª¨ë¸ ì „ë¬¸ê°€ê°€ ë ê±°ì—ìš”\")\n",
    ")\n",
    "print('OKT í’ˆì‚¬íƒœê¹… :',okt.pos(\"ì—´ì‹¬íˆ ì½”ë”©í•œ ë‹¹ì‹  , ë‚´ë…„ì—ëŠ” ì–¸ì–´ëª¨ë¸ ì „ë¬¸ê°€ê°€ ë ê±°ì—ìš”\"))\n",
    "print('OKT ëª…ì‚¬ì¶”ì¶œ :',okt.nouns(\"ì—´ì‹¬íˆ ì½”ë”©í•œ ë‹¹ì‹  , ë‚´ë…„ì—ëŠ” ì–¸ì–´ëª¨ë¸ ì „ë¬¸ê°€ê°€ ë ê±°ì—ìš”\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FDIu1SQVgl7d",
   "metadata": {
    "id": "FDIu1SQVgl7d"
   },
   "source": [
    "(3) ìì—°ì–´ì²˜ë¦¬ ì¸ì½”ë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WSGiOa1IVtss",
   "metadata": {
    "id": "WSGiOa1IVtss"
   },
   "outputs": [],
   "source": [
    "# ì› í•« ì¸ì½”ë”© ì‹¤ìŠµ\n",
    "\n",
    "# ì¼€ë¼ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì…ë ¥í•˜ê¸°\n",
    "text = \"ë‚˜ë‘ ì ì‹¬ ë¨¹ìœ¼ëŸ¬ ê°ˆë˜ ì ì‹¬ ë©”ë‰´ëŠ” í–„ë²„ê±° ê°ˆë˜ ê°ˆë˜ í–„ë²„ê±° ìµœê³ ì•¼\"\n",
    "\n",
    "# í† í¬ë‚˜ì´ì§• ë° ì¸ì½”ë”©í•˜ê¸°\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "print(text)\n",
    "print(' ë‹¨ì–´ì§‘í•© :',tokenizer.word_index, '\\n')\n",
    "\n",
    "# ë‹¤ë¥¸ í…ìŠ¤íŠ¸ì˜ ì¸ì½”ë”© í™•ì¸í•˜ê¸°\n",
    "sub_text = \"ì ì‹¬ ë¨¹ìœ¼ëŸ¬ ê°ˆë˜ ë©”ë‰´ëŠ” í–„ë²„ê±° ìµœê³ ì•¼\"\n",
    "\n",
    "encoded = tokenizer.texts_to_sequences([sub_text])[0]\n",
    "print(sub_text)\n",
    "print(encoded, '\\n')\n",
    "\n",
    "# ê° ì¸ë±ìŠ¤ë¥¼ ì› í•« ì¸ì½”ë”©í•˜ê¸°\n",
    "one_hot = to_categorical(encoded)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WcSdioA-8uFi",
   "metadata": {
    "id": "WcSdioA-8uFi"
   },
   "source": [
    "**6. í…ìŠ¤íŠ¸ë¥¼ ì´ìš©í•œ ì¶”ì²œì‹œìŠ¤í…œ ë§Œë“¤ê¸°**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aOmXGNP-hd-2",
   "metadata": {
    "id": "aOmXGNP-hd-2"
   },
   "outputs": [],
   "source": [
    "#1. í•„ìš”ë¼ì´ë¸Œë¦¬ë¼ì™€ í•™ìŠµë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "!pip install gensim\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import re\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "csv_path = \"/content/drive/MyDrive/Seoul_Fintech\"\n",
    "filename = \"202511_Book Recommand.csv\"\n",
    "\n",
    "full_path = os.path.join(csv_path, filename)\n",
    "print(\"ë¶ˆëŸ¬ì˜¬ íŒŒì¼ ê²½ë¡œ:\", full_path)\n",
    "\n",
    "df = pd.read_csv(full_path, encoding=\"utf-8-sig\")  # í•„ìš”í•˜ë©´ cp949 ë¡œ ë³€ê²½\n",
    "print('ì „ ì²´ ë¬¸ ì„œ ì˜ ìˆ˜ :', len(df))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-PgiArT4-J4w",
   "metadata": {
    "id": "-PgiArT4-J4w"
   },
   "outputs": [],
   "source": [
    "#2. í•™ìŠµë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°\n",
    "\n",
    "def _removeNonAscii(s):\n",
    "  return \"\".join(i for i in s if ord(i)<128)\n",
    "\n",
    "def make_lower_case(text):\n",
    "  return text.lower()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "  text = text.split()\n",
    "  stops = set(stopwords.words(\"english\"))\n",
    "  text = [w for w in text if not w in stops]\n",
    "  text = \" \".join(text)\n",
    "  return text\n",
    "\n",
    "def remove_html(text):\n",
    "  html_pattern = re.compile('<.*?>')\n",
    "  return html_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "  tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "  text = tokenizer.tokenize(text)\n",
    "  text = \" \".join(text)\n",
    "  return text\n",
    "\n",
    "df['cleaned'] = df['Desc'].apply(_removeNonAscii)\n",
    "df['cleaned'] = df.cleaned.apply(make_lower_case)\n",
    "df['cleaned'] = df.cleaned.apply(remove_stop_words)\n",
    "df['cleaned'] = df.cleaned.apply(remove_punctuation)\n",
    "df['cleaned'] = df.cleaned.apply(remove_html)\n",
    "df['cleaned'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adgDYpYomURv",
   "metadata": {
    "id": "adgDYpYomURv"
   },
   "outputs": [],
   "source": [
    "#3. ì •ì œëœ ë°ì´í„° í™•ì¸í•˜ê¸°\n",
    "\n",
    "df['cleaned'] = df['cleaned'].replace('', np.nan)\n",
    "df = df[df['cleaned'].notna()]\n",
    "\n",
    "print(' ì „ ì²´ ë¬¸ ì„œ ì˜ ìˆ˜ :',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DDfLIG29mZQ0",
   "metadata": {
    "id": "DDfLIG29mZQ0"
   },
   "outputs": [],
   "source": [
    "#4. ë¶ˆìš©ì–´ ì œê±°í•˜ê¸°\n",
    "\n",
    "# ì˜ì–´ stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# 1) Desc â†’ cleaned\n",
    "df[\"cleaned\"] = df[\"Desc\"].apply(clean_text)\n",
    "\n",
    "print(\"cleaned ì˜ˆì‹œ:\", df[\"cleaned\"].iloc[0])\n",
    "\n",
    "# 2) cleaned â†’ corpus\n",
    "corpus = [row.split() for row in df[\"cleaned\"]]\n",
    "print(\"corpus ë¬¸ì„œ ìˆ˜:\", len(corpus))\n",
    "print(\"ì²« ë¬¸ì„œ í† í° ì˜ˆì‹œ:\", corpus[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YknCV5YZmgOf",
   "metadata": {
    "id": "YknCV5YZmgOf"
   },
   "outputs": [],
   "source": [
    "#6. ë¯¸ë¦¬í•™ìŠµëœ ë°ì´í„°ì…‹ ì••ì¶•í’€ê¸°\n",
    "!unzip -o /content/googlenewsvectorsnegative300.zip -d /content\n",
    "\n",
    "# íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "!ls -lh /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PstLtpd7mjiS",
   "metadata": {
    "id": "PstLtpd7mjiS"
   },
   "outputs": [],
   "source": [
    "#7. Word2Vecëª¨ë¸ ì‚¬ì „í•™ìŠµ ëª¨ë¸ë¡œ íŒŒì¸íŠœë‹í•˜ê¸°\n",
    "google_bin_path = \"/content/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "# 1) Word2Vec ëª¨ë¸ ì´ˆê¸°í™”\n",
    "word2vec_model = Word2Vec(\n",
    "    vector_size=300,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4\n",
    ")\n",
    "# 2) ë‚´ corpusë¡œ vocab êµ¬ì¶•\n",
    "word2vec_model.build_vocab(corpus)\n",
    "# 3) GoogleNews ì‚¬ì „í•™ìŠµ Word2Vec ë¡œë“œ\n",
    "print(\"GoogleNews ë²¡í„° ë¡œë”© ì¤‘...\")\n",
    "google_kv = KeyedVectors.load_word2vec_format(\n",
    "    google_bin_path,\n",
    "    binary=True\n",
    ")\n",
    "print(\"GoogleNews ë²¡í„° ë¡œë”© ì™„ë£Œ!\")\n",
    "# 4) ê³µí†µ ë‹¨ì–´ë“¤ì— ëŒ€í•´ ì´ˆê¸° ê°€ì¤‘ì¹˜ ì£¼ì…\n",
    "common_cnt = 0\n",
    "for word in word2vec_model.wv.key_to_index.keys():\n",
    "    if word in google_kv.key_to_index:\n",
    "        word2vec_model.wv[word] = google_kv[word]\n",
    "        common_cnt += 1\n",
    "print(\"ì‚¬ì „í•™ìŠµ ë²¡í„°ë¡œ ì´ˆê¸°í™”ëœ ë‹¨ì–´ ìˆ˜:\", common_cnt)\n",
    "# 5) ë‚´ corpusë¡œ íŒŒì¸íŠœë‹\n",
    "word2vec_model.train(\n",
    "    corpus,\n",
    "    total_examples=word2vec_model.corpus_count,\n",
    "    epochs=15\n",
    ")\n",
    "print(\"Word2Vec + GoogleNews ì‚¬ì „í•™ìŠµ ê¸°ë°˜ íŒŒì¸íŠœë‹ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gxJEdO4fmmEt",
   "metadata": {
    "id": "gxJEdO4fmmEt"
   },
   "outputs": [],
   "source": [
    "#8. ë¬¸ì„œ ë²¡í„°ìˆ˜ ê³„ì‚°í•˜ê¸°\n",
    "\n",
    "def get_document_vectors(document_series):\n",
    "    document_embedding_list = []\n",
    "\n",
    "    for line in document_series:\n",
    "        words = str(line).split()\n",
    "        doc2vec = None\n",
    "        count = 0\n",
    "\n",
    "        for word in words:\n",
    "\n",
    "            if word in word2vec_model.wv.key_to_index:\n",
    "                count += 1\n",
    "                if doc2vec is None:\n",
    "                    doc2vec = word2vec_model.wv[word]\n",
    "                else:\n",
    "                    doc2vec = doc2vec + word2vec_model.wv[word]\n",
    "\n",
    "        # 1ê°œ ì´ìƒ ë‹¨ì–´ê°€ ì¡´ì¬í•˜ë©´ í‰ê· \n",
    "        if doc2vec is not None and count > 0:\n",
    "            document_embedding_list.append(doc2vec / count)\n",
    "        else:\n",
    "            # ë‹¨ì–´ê°€ í•˜ë‚˜ë„ Word2Vec vocabì— ì—†ìœ¼ë©´ 0 ë²¡í„° ë„£ê¸°\n",
    "            document_embedding_list.append(np.zeros(word2vec_model.vector_size))\n",
    "\n",
    "    return document_embedding_list\n",
    "\n",
    "document_embedding_list = get_document_vectors(df['cleaned'])\n",
    "print(\"ë¬¸ì„œ ë²¡í„°ì˜ ìˆ˜:\", len(document_embedding_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SbvX3A1gmqYL",
   "metadata": {
    "id": "SbvX3A1gmqYL"
   },
   "outputs": [],
   "source": [
    "#9. ë¬¸ì„œ ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚°í•˜ê¸°\n",
    "# document_embedding_list: ê° ë¬¸ì„œì˜ ë²¡í„° (ê¸¸ì´: ë¬¸ì„œ ìˆ˜)\n",
    "doc_matrix = np.vstack(document_embedding_list)   # (N, 300) í˜•íƒœë¡œ ìŒ“ê¸°\n",
    "\n",
    "cosine_similarities = cosine_similarity(doc_matrix, doc_matrix)\n",
    "print(\"ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ì˜ í¬ê¸°:\", cosine_similarities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BrcLph46mtl_",
   "metadata": {
    "id": "BrcLph46mtl_"
   },
   "outputs": [],
   "source": [
    "#10. ì¶”ì²œë¡œì§ êµ¬í˜„ ë° í‘œí˜„êµ¬ì¡°ì™€ ì¶”ì²œí•˜ê¸°\n",
    "def recommendations(title, topn=5):\n",
    "    # ì œëª©ê³¼ ì´ë¯¸ì§€ ë§í¬ë§Œ ì‚¬ìš©í•˜ëŠ” DataFrame\n",
    "    books = df[['title', 'image_link']]\n",
    "    # title â†’ index ë§¤í•‘ (ì¤‘ë³µ ì œê±°)\n",
    "    indices = pd.Series(df.index, index=df['title']).drop_duplicates()\n",
    "    # ì œëª©ì´ ë°ì´í„°ì— ì—†ëŠ” ê²½ìš° ì²˜ë¦¬\n",
    "    if title not in indices:\n",
    "        print(\"í•´ë‹¹ ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\", title)\n",
    "        return\n",
    "\n",
    "    # ì…ë ¥ëœ ì±…ì˜ ì¸ë±ìŠ¤\n",
    "    idx = indices[title]\n",
    "    # í•´ë‹¹ ì±…ê³¼ ëª¨ë“  ì±…ì˜ ìœ ì‚¬ë„ ì ìˆ˜\n",
    "    sim_scores = list(enumerate(cosine_similarities[idx]))\n",
    "    # ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # ìê¸° ìì‹ (0ë²ˆì§¸)ì„ ì œì™¸í•˜ê³  ìƒìœ„ topnê°œ ì„ íƒ\n",
    "    sim_scores = sim_scores[1:topn+1]\n",
    "\n",
    "    # ì¶”ì²œí•  ì±… ì¸ë±ìŠ¤\n",
    "    book_indices = [i[0] for i in sim_scores]\n",
    "    # ì¶”ì²œ ê²°ê³¼ DataFrame\n",
    "    recommend = books.iloc[book_indices].reset_index(drop=True)\n",
    "    # ì´ë¯¸ì§€ì™€ ì œëª© ì¶œë ¥\n",
    "    fig = plt.figure(figsize=(4 * topn, 6))\n",
    "    for index, row in recommend.iterrows():\n",
    "        try:\n",
    "            response = requests.get(row['image_link'])\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            ax = fig.add_subplot(1, topn, index + 1)\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(row['title'])\n",
    "            ax.axis('off')\n",
    "        except Exception as e:\n",
    "            print(\"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨:\", row['title'], \"| ì˜¤ë¥˜:\", e)\n",
    "\n",
    "    plt.show()\n",
    "    return recommend\n",
    "\n",
    "recommendations(\"The Da Vinci Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1oGqZb3zmxdZ",
   "metadata": {
    "id": "1oGqZb3zmxdZ"
   },
   "outputs": [],
   "source": [
    "#11. ë” ì¶”ì²œí•´ë³´ê¸°\n",
    "recommendations(\"Angels & Demons\")\n",
    "recommendations(\"The Hobbit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XmGsBwGsm5zZ",
   "metadata": {
    "id": "XmGsBwGsm5zZ"
   },
   "source": [
    "**7. íŠ¸ëœìŠ¤í¬ë¨¸ìŠ¤**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6nXcncC_nAcY",
   "metadata": {
    "id": "6nXcncC_nAcY"
   },
   "outputs": [],
   "source": [
    "#1. ì–´íƒ ì…˜ ì‹¤ìŠµí•˜ê¸°\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. ë‹¨ì–´ ì‚¬ì „ ë§Œë“¤ê¸°\n",
    "vocab = [\"ë‚˜\", \"ëŠ”\", \"ì˜¤ëŠ˜\", \"ì„œìš¸\", \"í•€í…Œí¬\", \"ì•„ì¹´ë°ë¯¸\", \"ì—\", \"ê°”ë‹¤\"]\n",
    "stoi = {w: i for i, w in enumerate(vocab)}   # string â†’ index\n",
    "itos = {i: w for w, i in stoi.items()}       # index â†’ string\n",
    "\n",
    "# 2. ì˜ˆì‹œ ë¬¸ì¥ ì…ë ¥\n",
    "sentence = [\"ë‚˜\", \"ëŠ”\", \"ì˜¤ëŠ˜\", \"ì„œìš¸\", \"í•€í…Œí¬\", \"ì•„ì¹´ë°ë¯¸\", \"ì—\", \"ê°”ë‹¤\"]\n",
    "ids = torch.tensor([stoi[w] for w in sentence])   # (seq_len,)\n",
    "\n",
    "print(\"ë¬¸ì¥:\", sentence)\n",
    "print(\"í† í° ID:\", ids.tolist())\n",
    "\n",
    "# 2. ì„ë² ë”© (word â†’ vector)\n",
    "d_model = 8   # ì„ë² ë”© ì°¨ì›\n",
    "embedding = torch.nn.Embedding(len(vocab), d_model)\n",
    "x = embedding(ids)\n",
    "print(\"\\nì„ë² ë”© shape:\", x.shape)\n",
    "\n",
    "\n",
    "# 3. Q, K, V ë§Œë“¤ê¸° (Self-Attention ê¸°ë³¸)\n",
    "W_Q = torch.nn.Linear(d_model, d_model, bias=False)\n",
    "W_K = torch.nn.Linear(d_model, d_model, bias=False)\n",
    "W_V = torch.nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "Q = W_Q(x)\n",
    "K = W_K(x)\n",
    "V = W_V(x)\n",
    "\n",
    "# 4. Scaled Dot-Product Attention\n",
    "scores = Q @ K.T / (d_model ** 0.5)   # (6 Ã— 6)\n",
    "attn_weights = F.softmax(scores, dim=-1)\n",
    "attn_output = attn_weights @ V\n",
    "\n",
    "print(\"\\n[ì–´í…ì…˜ score í–‰ë ¬ (ì› ì ìˆ˜)]\")\n",
    "print(scores)\n",
    "\n",
    "print(\"\\n[ì–´í…ì…˜ weight í–‰ë ¬ (softmax í›„ í™•ë¥ )]\")\n",
    "print(attn_weights)\n",
    "\n",
    "# 5. ë‹¨ì–´ë³„ë¡œ ì–´íƒ ì…˜ ë¶„ì„\n",
    "print(\"\\n============== ë‹¨ì–´ë³„ ì–´í…ì…˜ í•´ì„ ==============\")\n",
    "for i, qword in enumerate(sentence):\n",
    "    print(f\"\\nğŸ” '{qword}'(ì´)ê°€ ë¬¸ì¥ì„ ë°”ë¼ë³´ëŠ” ì‹œì„ :\")\n",
    "    for j, kword in enumerate(sentence):\n",
    "        print(f\"   {qword:>4s} â†’ {kword:<4s}:  {attn_weights[i, j].item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lErZhbSSnIBj",
   "metadata": {
    "id": "lErZhbSSnIBj"
   },
   "source": [
    "** ëŸ°íƒ€ì„ ìœ í˜• ë°”ê¾¸ê¸° : CPU -> GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l1YXroBEnQmh",
   "metadata": {
    "id": "l1YXroBEnQmh"
   },
   "outputs": [],
   "source": [
    "# BLIP ì˜¤í”ˆì†ŒìŠ¤ë¡œ ì´ë¯¸ì§€ì— ìº¡ì…˜ ìƒì„±í•˜ê¸°\n",
    "\n",
    "# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install -q transformers pillow torch\n",
    "import os\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ë“œë¼ì´ë¸Œ ë‹¤ì‹œ ë§ˆìš´íŠ¸í•˜ê¸°\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# 2. ì´ë¯¸ì§€ ë¡œë“œí•˜ê¸°\n",
    "BASE_DIR = \"/content/drive/MyDrive/Seoul_Fintech2\"\n",
    "image_files = [\n",
    "    \"IMG1.png\",\n",
    "    \"IMG2.png\",\n",
    "    \"IMG3.png\",\n",
    "    \"IMG4.png\",\n",
    "    \"IMG5.png\",\n",
    "    \"IMG6.png\",\n",
    "]\n",
    "image_paths = [os.path.join(BASE_DIR, f) for f in image_files]\n",
    "image_paths\n",
    "\n",
    "\n",
    "# 3. BLIP ëª¨ë¸ë¡œë“œ í•˜ê¸°\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"ì‚¬ìš© ë””ë°”ì´ìŠ¤:\", device)\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\"\n",
    ").to(device)\n",
    "\n",
    "# 4. ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± ë° ì‹œê°í™”\n",
    "def generate_caption(image_path: str) -> str:\n",
    "    \"\"\"í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì— ëŒ€í•´ ìº¡ì…˜ í•œ ì¤„ ìƒì„±\"\"\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=20,   # ìº¡ì…˜ ê¸¸ì´ ì¡°ì ˆ\n",
    "            num_beams=5          # ë¹”ì„œì¹˜(ì¡°ê¸ˆ ë” ì•ˆì •ì ì¸ ë¬¸ì¥)\n",
    "        )\n",
    "    caption = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return img, caption\n",
    "\n",
    "# ì´ë¯¸ì§€ì— ëŒ€í•´ ìˆœì„œëŒ€ë¡œ ìº¡ì…˜ ìƒì„±\n",
    "results = []\n",
    "for p in image_paths:\n",
    "    img, cap = generate_caption(p)\n",
    "    results.append((os.path.basename(p), img, cap))\n",
    "\n",
    "# ì´ë¯¸ì§€ íŒë‹¨ ê²°ê³¼ ì‹œê°í™”\n",
    "for name, img, cap in results:\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"{name}\\n{cap}\", fontsize=10)\n",
    "    plt.show()\n",
    "    print(f\"[{name}]  caption: {cap}\")\n",
    "    print(\"-\" * 60)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1LGPv4HZbX2JpInblfyxMYktxVOQonfFg",
     "timestamp": 1763362844591
    },
    {
     "file_id": "https://github.com/cduck243/Seoul_FinData/blob/main/SEOUL_FIN_SHARE_CODE.ipynb",
     "timestamp": 1762913724719
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
